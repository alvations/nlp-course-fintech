{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# План на сегодня\n",
    "1. Как разбить текст на слова? \n",
    "2. Как посчитать количество слов?\n",
    "3. Все ли слова нужны? Удаление стоп-слов.\n",
    "4. Как определить часть речи слова. \n",
    "\n",
    "Инструменты: nltk, pymorphy2, pymystem3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация и подсчет количества слов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сколько слов в этом предложении?\n",
    "* На дворе трава, на траве дрова, не руби дрова на траве двора.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Токен и тип\n",
    "\n",
    "**  Тип **  – уникальное слово из текста\n",
    "\n",
    "** Токен **  – тип и его позиция в тексте\n",
    "\n",
    "** Лексема **  – абстрактная сущность слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Обозначения \n",
    "$N$ = число токенов\n",
    "\n",
    "$V$ – словарь (все типы)\n",
    "\n",
    "$|V|$ = количество типов в словаре (то есть число уникальных слов)\n",
    "\n",
    "** Как связаны $N$ и $|V|$?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Закон Ципфа\n",
    "\n",
    "\n",
    "**В любом достаточно большом тексте ранг типа обратно пропорционален его частоте: $f = \\frac{a}{r}$ **\n",
    "\n",
    "$f$ – частота типа, $r$  – ранг типа, $a$  – параметр, для славянских языков – около 0.07\n",
    "\n",
    "Иными словами, если расмотреть список слов, упорядоченный по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"Zipf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Закон Хипса\n",
    "\n",
    "**С увеличением длины текста (количества токенов), количество типов увеличивается в соответствии с законом: $|V| = K*N^b$ **\n",
    "\n",
    "\n",
    "$N$  –  число токенов, $|V|$  – количество типов в словаре, $K, b$  –  параметры, обычно $K \\in [10,100], b \\in [0.4, 0.6]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"Hips.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверим на практике!\n",
    "\n",
    "## Анализ  сообщений vk.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Тексты из пабликов госслужб из ВК\n",
    "df = pd.read_csv('vk_texts_with_sources.csv', usecols = ['text', 'source'])\n",
    "\n",
    "df.text.dropna(inplace = True)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что за данные?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные из госпабликов ВК. В поле source записан id группы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.source.unique())\n",
    "for group in df.source.unique():\n",
    "    print('https://vk.com/' + group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительный анализ коллекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Средняя длина текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = df.text.apply(len)\n",
    "len_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество текстов из разных пабликов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "counts = df.source.value_counts()\n",
    "values = counts.tolist()\n",
    "labels = counts.index.tolist()\n",
    "\n",
    "\n",
    "y_pos = np.arange(len(labels))\n",
    "\n",
    " \n",
    "plt.bar(y_pos, values, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, range(len(labels)))\n",
    "\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Длины текстов (в символах)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "length = len_data[len_data < 10000].tolist()\n",
    "\n",
    "n, bins, patches = ax.hist(length)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most typical length:', round(len_data.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация\n",
    "\n",
    "Используем регулярные выражения, чтобы разбить тексты на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(\"[А-Яа-я]+\")\n",
    "\n",
    "#Оставляем только слова, удаляем мусор.\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "df.text = df.text.str.lower()\n",
    "df.text = df.text.apply(words_only)\n",
    "\n",
    "df.text.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самые частые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "n_types = []\n",
    "n_tokens = []\n",
    "tokens = []\n",
    "fd = FreqDist()\n",
    "for index, row in df.iterrows():\n",
    "    tokens = row['text'].split()\n",
    "    fd.update(tokens)\n",
    "    n_types.append(len(fd))\n",
    "    n_tokens.append(sum(fd.values()))\n",
    "for i in fd.most_common(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Вопрос на подумать **\n",
    "\n",
    "Несут ли эти слова смысловую нагрузку? Много ли информации они содержат?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверим законы на практике"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Закон Ципфа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = list(fd.values())\n",
    "freqs = sorted(freqs, reverse = True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(freqs[:300], range(300))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Вопрос на подумать ** Совпадает ли это с графиком? Выполняется ли закон Ципфа?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Закон Хипса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_types, n_tokens)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Вопрос на подумать ** Совпадает ли это с графиком? Выполняется ли закон Хипса?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Вопрос на подумать ** Чему равно $b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подбираем кривизну кривой\n",
    "n_types = np.array(n_types)\n",
    "b_ = ## \n",
    "plt.plot(n_types, n_types ** b_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфологический анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задачи морфологического анализа\n",
    "* Разбор слова — определение нормальной формы  (леммы), основы (стема) и грамматических характеристик слова\n",
    "* Синтез слова — генерация слова по заданным грамматическим характеристикам\n",
    "\n",
    "\n",
    "### Морфологический процессор – инструмент морфологического анализа\n",
    "* Морфологический словарь \n",
    "* Морфологический анализатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация \n",
    "У каждого слова есть *лемма* (нормальная форма): \n",
    "\n",
    "* кошке, кошку, кошкам, кошкой $\\implies$ кошка\n",
    "* бежал, бежит, бегу $\\implies$  бежать\n",
    "* белому, белым, белыми $\\implies$ белый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Действительно, на его лице не отражалось никаких чувств – ни проблеска сочувствия не было на нем, а ведь боль просто невыносима'\n",
    "sent2 = 'У страха глаза велики .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инструменты:\n",
    "\n",
    "Два основных инструмента морфологического анализа:\n",
    "\n",
    "** 1) библиотека pymorphy2 и анализатор MorphAnalyzer **\n",
    "\n",
    "Представляет собой словарь вида.\n",
    "\n",
    "    dict[word] = lemma \n",
    "    \n",
    "Не умеет разрешать омонимию.\n",
    "    \n",
    "** 2) библиотека mystem и анализатор Mystem **\n",
    "\n",
    "Разработка Яндекса.\n",
    "\n",
    "Главное достоинство: умеет разрешать омонимию.\n",
    "\n",
    "Про Mystem можно почитать в статье I. Segalovich (2003), A fast morphological algorithm with unknown word guessing induced by a dictionary for a web search engine:\n",
    "\n",
    "\n",
    "https://cache-mskstoredata11.cdn.yandex.net/download.yandex.ru/company/iseg-las-vegas.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "m = MorphAnalyzer()\n",
    "lemmas1 = [m.parse(word)[0].normal_form for word in sent1.split()]\n",
    "print(' '.join(lemmas1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mystem - запрещен использование проприетарная разраббез оплаты разботка Яндекса\n",
    "#умеет разрешать омонимию (у страха глаза велики)\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "lemmas2 = m.lemmatize(sent1)\n",
    "print(''.join(lemmas2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Снятие омонимии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Омонимия ** - совпадение слов разных по смыслу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "lemmas1 = [m.parse(word)[0].normal_form for word in sent2.split()]\n",
    "print(' '.join(lemmas1))\n",
    "\n",
    "m = Mystem()\n",
    "lemmas2 = m.lemmatize(sent2)\n",
    "print(''.join(lemmas2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Основа слова ** - неизменяемая часть слова, которая выражает его лексическое значение.\n",
    "\n",
    "** Стемминг ** - процесс нахождения лексической основы для заданного исходного слова.\n",
    "\n",
    "$word = stem + affixes$\n",
    "\n",
    "$affixes = preffixes + suffixes + interfixes + others$\n",
    "\n",
    "В русском языке мы чаще всего хотим избавиться от суффиксов.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм Портера\n",
    "\n",
    "* популярный алгоритм стемминга\n",
    "\n",
    "* последовательно применяет ряд правил, отсекая суффиксы и окончания\n",
    "\n",
    "* работает быстро, но допускает ошибки\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ссылки:\n",
    "\n",
    "Porter's homepage https://tartarus.org/martin/PorterStemmer/\n",
    "\n",
    "Original paper https://www.emeraldinsight.com/doi/abs/10.1108/eb046814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "stemmer = RussianStemmer()\n",
    "words = ['распределение', 'приставить', 'сделала', 'словообразование']\n",
    "for w in words:\n",
    "    stem = stemmer.stem(w)\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 типа ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1-ый вид ошибки ** \n",
    "Разные слова приводятся к одной основе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "stemmer = RussianStemmer()\n",
    "words = ['белый','белье']\n",
    "for w in words:\n",
    "    stem = stemmer.stem(w)\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2-ый вид ошибки ** \n",
    "Слова с одинаковыми основами приводятся к разным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "stemmer = RussianStemmer()\n",
    "words = ['трудный','трудность']\n",
    "for w in words:\n",
    "    stem = stemmer.stem(w)\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3-ый вид ошибки ** \n",
    "Не удаляется приставка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "stemmer = RussianStemmer()\n",
    "words = ['быстрый','побыстрее']\n",
    "for w in words:\n",
    "    stem = stemmer.stem(w)\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбор слова \n",
    "\n",
    "** Граммема ** - грамматическое значение, понимаемое как один из элементов грамматической категории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'семью'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "parsed_word = m.parse(word1)\n",
    "parsed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "parsed_word = m.analyze(word1)\n",
    "parsed_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что в имени твоем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = ### Enter your name here  \n",
    "m = Mystem()\n",
    "parsed_word = m.analyze(your_name)\n",
    "parsed_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_parse = parsed_word[0]\n",
    "\n",
    "grammema = name_parse['analysis'][0]['gr']\n",
    "print(grammema)\n",
    "if re.search('имя', grammema):\n",
    "    print('It is really a name.')\n",
    "    \n",
    "if re.search('жен', grammema):\n",
    "    print('You are female.')\n",
    "elif re.search('муж', grammema):\n",
    "    print('You are male.')\n",
    "else:\n",
    "    print('Omm... Could not recognise you...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пришло время писать свой код!\n",
    "\n",
    "### Задание\n",
    "\n",
    "Найдите в списке персонажей романа \"Война и мир\"  (task.txt) все уникальные  женские имена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_file = open(\"task.txt\", \"r\", encoding='utf-8')\n",
    "lines = text_file.readlines()\n",
    "\n",
    "print(len(lines))\n",
    "for line in lines[1:50]:\n",
    "    print(line)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первичная обработка текстов\n",
    "\n",
    "### Еще раз загрузим те же самые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vk_texts_with_sources.csv', usecols = ['text', 'source'])\n",
    "df.text.dropna(inplace = True)\n",
    "\n",
    "#WARNING! Если вы используете WINDOWS, то Mystem() может работать медленно!\n",
    "#Если не хотите долго ждать, оставьте лишь часть данных!\n",
    "\n",
    "#Раскомментить данную строчку для пользователей Windows\n",
    "# df = df.head(100)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords = stopwords.words('russian') + ['это', 'наш' , 'тыс', 'млн', 'млрд', 'также',  'т', 'д']\n",
    "def  remove_stopwords(text, mystopwords = mystopwords):\n",
    "    try:\n",
    "        return \" \".join([token for token in text.split() if not token in mystopwords])\n",
    "    except:\n",
    "        return \"\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystoplemmas = ['который','прошлый','сей', 'свой', 'наш', 'мочь']\n",
    "def  remove_stoplemmas(text, mystoplemmas = mystoplemmas):\n",
    "    try:\n",
    "        return \" \".join([token for token in text.split() if not token in mystoplemmas])\n",
    "    except:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(remove_stopwords) \n",
    "df.text = df.text.apply(lemmatize)\n",
    "df.text = df.text.apply(remove_stoplemmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmata = []\n",
    "for index, row in df.iterrows():\n",
    "    lemmata += row['text'].split()\n",
    "fd = FreqDist(lemmata)\n",
    "for i in fd.most_common(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонус. Демо Natasha\n",
    "\n",
    "Natasha - библиотека для поиска и извлечения именованных сущностей (Named-entity recognition) из текстов на русском языке. На данный момент разбираются упоминания персон, даты и суммы денег."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import *\n",
    "\n",
    "\n",
    "text = 'Экс-президента Франции Николя Саркози задержали по делу о финансировании его избирательной кампании 2007 года, \\\n",
    "передает французская газета Monde. Сейчас, по данным издания, политик находится в помещениях судебной полиции в Нантере. \\\n",
    "Бывший президент Франции пробудет под стражей минимум 48 часов. \\\n",
    "Весной 2012 года французское издание Mediapart опубликовало документы, в которых говорилось о передаче ливийским режимом \\\n",
    "50 млн евро на нужды президентской кампании Саркози. Судебные разбирательства по этому делу продолжаются во Франции до сих пор.'\n",
    "\n",
    "#text = 'Под Нижним Новгородом передали Нижнему Новгороду передали 100 евро'\n",
    "\n",
    "extractors = [NamesExtractor(), PersonExtractor(), DatesExtractor(), LocationExtractor(), OrganisationExtractor(), MoneyExtractor()]\n",
    "for extractor in extractors:\n",
    "    matches = extractor(text)\n",
    "    for match in matches:\n",
    "        print(match.span, match.fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наташа выделяет именованные сущности на основе правил, в связи с чем она допускает ошибки.\n",
    "\n",
    "### Пример неправильного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import *\n",
    "\n",
    "# Отрывок статьи из Википедии о Хараки Мураками\n",
    "\n",
    "text = 'Харуки Мураками (12 января 1949 года, Киото) — японский писатель и переводчик. \\\n",
    "        Его книги переведены на 50 языков и являются бестселлерами как в Японии, \\\n",
    "        так и за пределами его родной страны. '\n",
    "\n",
    "#text = 'Под Нижним Новгородом передали Нижнему Новгороду передали 100 евро'\n",
    "\n",
    "extractors = [NamesExtractor(), PersonExtractor(), DatesExtractor(), LocationExtractor(), OrganisationExtractor(), MoneyExtractor()]\n",
    "for extractor in extractors:\n",
    "    matches = extractor(text)\n",
    "    for match in matches:\n",
    "        print(match.span, match.fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
